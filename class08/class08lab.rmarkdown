---
title: "Class 08 Mini Project"
author: "Andres Sandoval"
format: gfm
---


In today's project we will complete analysis using clustering and PCA. 

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data. 

## Data Import


```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
dim(wisc.df)
```


The diagnosis column is essentially the answer for our analysis. In that sense, we want to remove it from the data set. With this, we need to make a new data frame that does not include the first column, or diagnosis column. However, set up a diagnosis vector containing the first column for later usage. 


```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```



## Exploratory Data Analysis 

> Q1. How many observations (patients = rows) are in this dataset?


```{r}
nrow(wisc.data)
```

There are 569 patients in this data set. 


> Q2. How many of the observations have a malignant diagnosis?


```{r}
table(wisc.df$diagnosis)
```

There are 212 malignant diagnosis patients. 


> Q3. How many variables/features in the data are suffixed with _mean?

First find the column names

```{r}
colnames(wisc.data)
```


Next I need to search within the column names for "_mean" pattern. The `grep()` function might help here. You need to input a vector, not a data frame when using `grep()`. This function is broken down by `grep("phrase of interest", vector)`

```{r}
grep("_mean", colnames(wisc.data) )
#This tells me where the "_mean" was found. Need to use length()
length( grep("_mean", colnames(wisc.data) ) )
```

There are 10 variables/features in this data set that have "_mean"

> Q. How many  dimensions are in this dataset?


```{r}
dim(wisc.data)
```



## Principal Component Analysis

First do we need to scale the data before PCA or not. 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
```

```{r}
round( apply(wisc.data, 2, sd), 2)
```



Looks like we need to scale. By scaling it allows us to avoid one set of numbers/columns to dominant the analysis. 

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale =TRUE)
# Look at summary of results
summary(wisc.pr)
```



> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%


> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Three PCs are required to describe at least 70% of the original variance. 

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven PCs are required to describe at least 90% of the original variance. 


## Interpreting PCA results

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?


```{r}
biplot(wisc.pr)
```

This plot is hard for me to understand because all the data is concentrated into one area. The points are hard to read and the graph itself is illogical for analysis. 

Therefore we need to generate a clearer graph. In this case generate a plot of PC1 vs PC2. 

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```



> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
# Repeat for components 1 and 3
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

The plots are very similar with teh differences being mainly within the range of the axis. 


```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```



# Variance Explained

We can get this from the output of the summary function

```{r}
summary(wisc.pr)
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


*Scree Plot 1*

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


*Scree Plot 2*

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```



#Examine the PC loadings 

How much do the original variables contribute to the new PCs that we have calculated? To get this we can look at the `$rotation` componenet of the returned PCA object. 


```{r}
head( wisc.pr$rotation[,1:3] )
```


>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?


```{r}
wisc.pr$rotation["concave.points_mean",1]
```



there is a complicated mix of variables that go together to make up PC1 - i.e. there are many of the original variables that together contribute highly to PC1. 

```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings) + aes(PC1, rownames(loadings)) + geom_col()
```



>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

Five PCs are needed to explain 80% of the variance in this data. 

# Hierarchical Clustering


```{r}
# Scale the wisc.data data using the "scale()" function and then plot.
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```

This cluster is a bad representation. We need to cut this tree to yield our cluster membership vector using the `cutree()` function. 



```{r}
grps <- cutree(wisc.hclust, h= 19)
table(grps)
```

```{r}
table(grps, diagnosis)
```


>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?


```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)

table (cutree(wisc.hclust, k= 4))
```


> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?


```{r}
grps <- cutree(wisc.hclust, h= 13)
table(grps, diagnosis)
```



> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.


```{r}
plot( hclust(data.dist, method = "complete") )
plot( hclust(data.dist, method ="ward.D2") )
plot( hclust(data.dist, method ="single") )
plot( hclust(data.dist, method ="mcquitty") )
```


Personally, I like the ward.D2 results. This is because there is a wider spread of clusters. 


# Combine methods: PCA and HCLUST

My PCA results were interesting as they showed a separation of M and B samples along PC1. 

I want to cluster my PCA results - that is use the `wisc.pr$x` as input to `hclust()`. 


```{r}
d <- dist(wisc.pr$x[,1:3])
```

```{r}
wisc.pr.hclust <- hclust(d, method="ward.D2")
```


And my tree result figure

```{r}
plot(wisc.pr.hclust)
```



Let's cut this tree into two groups/clusters

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```





> Q15. How well do the two clusters separate the M and B diagnoses? 


```{r}
table(grps, diagnosis)
```

```{r}
(179+333)/nrow(wisc.data)
```

